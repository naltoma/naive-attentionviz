{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AttentionViz（の一部）をBERTで再現してみる\n",
    "[AttentionViz: A Global View of Transformer Attention](https://catherinesyeh.github.io/attn-docs/)の「5.1.1 Vector Nomalization」を実装し、相関が高くなる定数を求めた上でQ,Kベクトル正規化。それをPCAで描画してみた。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTからQ, K, Vの重みベクトルを取得に向けて1（隠れ層、アテンション取得）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tnal/.venv/opencalm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(input_ids[0])=11\n",
      "tokens=['[CLS]', 'the', 'brown', 'cap', '##y', '##bara', 'is', 'sleeping', 'now', '.', '[SEP]']\n",
      "len(hidden_states)=13\n",
      "hidden_states[0].shape=torch.Size([1, 11, 768])\n",
      "len(attention)=12\n",
      "attention[0].shape=torch.Size([1, 12, 11, 11])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from torch import nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# モデルの容易\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name, output_attentions=True, output_hidden_states=True)\n",
    "\n",
    "# テキストを入力としてモデルを実行\n",
    "text = \"The brown capybara is sleeping now.\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "print(f'{len(input_ids[0])=}') # 11 tokens\n",
    "print(f'{tokens=}')\n",
    "\n",
    "# モデルの各層の出力を取得\n",
    "outputs = model(input_ids)\n",
    "hidden_states = outputs[\"hidden_states\"]\n",
    "print(f'{len(hidden_states)=}') # input layer + 12 layers = 13\n",
    "print(f'{hidden_states[0].shape=}') # torch.Size([1, 11, 768]), [sequence_size, token_num, dims]\n",
    "\n",
    "# 各層のアテンションを取得（動作確認用）\n",
    "attention = outputs[\"attentions\"]\n",
    "print(f'{len(attention)=}') # 12 layers\n",
    "print(f'{attention[0].shape=}') # torch.Size([1, 12, 11, 11]), [sequence_size, heads, token_num, token_num]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTからQ, K, Vの重みベクトルを取得に向けて2（レイヤー0でQ, K, V取得、アテンション算出）\n",
    "Q, K, Vの取得は[Source code for transformers.modeling_bert](https://huggingface.co/transformers/v3.2.0/_modules/transformers/modeling_bert.html)の BertSelfAttention.forward を参考にしました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_layer.shape=torch.Size([1, 12, 11, 64])\n",
      "key_layer.shape=torch.Size([1, 12, 11, 64])\n",
      "value_layer.shape=torch.Size([1, 12, 11, 64])\n",
      "torch.allclose(attention_probs, attention[layer_index])=True\n"
     ]
    }
   ],
   "source": [
    "layer_index = 0 # 隠れ層1層目だけを対象とする。\n",
    "\n",
    "# Q, K, Vの重みベクトル取得\n",
    "mixed_query_layer = model.encoder.layer[layer_index].attention.self.query(outputs[\"hidden_states\"][layer_index])\n",
    "mixed_key_layer = model.encoder.layer[layer_index].attention.self.key(outputs[\"hidden_states\"][layer_index])\n",
    "mixed_value_layer = model.encoder.layer[layer_index].attention.self.value(outputs[\"hidden_states\"][layer_index])\n",
    "\n",
    "query_layer = model.encoder.layer[layer_index].attention.self.transpose_for_scores(mixed_query_layer)\n",
    "key_layer = model.encoder.layer[layer_index].attention.self.transpose_for_scores(mixed_key_layer)\n",
    "value_layer = model.encoder.layer[layer_index].attention.self.transpose_for_scores(mixed_value_layer)\n",
    "print(f'{query_layer.shape=}') # torch.Size([1, 12, 11, 64]), [sequence_size, heads, token_num, dims]\n",
    "print(f'{key_layer.shape=}') # same\n",
    "print(f'{value_layer.shape=}') # same\n",
    "\n",
    "# アテンション求めてみる\n",
    "attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "attention_head_size = model.encoder.layer[layer_index].attention.self.attention_head_size\n",
    "attention_scores = attention_scores / math.sqrt(attention_head_size)\n",
    "attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "\n",
    "# 実際のアテンションと計算結果がおおよそ等しいこと（=Q,K,Vが正しく取れてること）を確認\n",
    "print(f'{torch.allclose(attention_probs, attention[layer_index])=}') # True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ベクトル正規化\n",
    "定数a,cを求めるために総当り法チックにやってるのだけど、他に良い方法あるよね。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal_a=-0.5, optimal_c=-2.0, optimal_corr=0.8910502854269778\n",
      "optimal_a=-0.5, optimal_c=-1.5555555555555556, optimal_corr=0.8910502954454634\n"
     ]
    }
   ],
   "source": [
    "def normalize_vectors(query_layer, key_layer, a, c):\n",
    "    \"\"\"論文5.1.1 Vector Nomalization（定数a,cを元に正規化）\n",
    "    query_layer, key_layerは、この前に取得した重みベクトル。\n",
    "    a, cは定数（スカラ）。\n",
    "    \"\"\"\n",
    "    # before\n",
    "    num_sequence, num_heads, num_tokens, dims = query_layer.shape\n",
    "    queries_reshaped = torch.reshape(query_layer, (-1,dims))\n",
    "    keys_reshaped = torch.reshape(key_layer, (-1,dims))\n",
    "\n",
    "    # after\n",
    "    shifted_keys = keys_reshaped + a\n",
    "    scaled_queries = queries_reshaped * c\n",
    "    scaled_keys = shifted_keys * (1.0 / c)\n",
    "\n",
    "    # reshape to orig\n",
    "    normalized_query = torch.reshape(scaled_queries, (num_sequence, num_heads, num_tokens, dims))\n",
    "    normalized_key = torch.reshape(scaled_keys, (num_sequence, num_heads, num_tokens, dims))\n",
    "\n",
    "    return normalized_query, normalized_key\n",
    "\n",
    "def calc_dot_product(query_layer, key_layer):\n",
    "    \"\"\"tensor同士の内積。\n",
    "    dims次元ベクトルに変換し、内積を求めている。\n",
    "    \"\"\"\n",
    "    # dims次元ベクトルに変換\n",
    "    dims = query_layer.shape[-1]\n",
    "    reshaped_queries = torch.reshape(query_layer, (-1,dims)).detach().numpy()\n",
    "    reshaped_keys = torch.reshape(key_layer, (-1,dims)).detach().numpy()\n",
    "\n",
    "    # 内積\n",
    "    dot_product = np.sum(reshaped_queries * reshaped_keys, axis=1)\n",
    "    return dot_product\n",
    "\n",
    "def calc_cosine_distance(query_layer, key_layer):\n",
    "    \"\"\"tensor同士のコサイン距離。\n",
    "    dims次元ベクトルに変換し、コサイン距離を求めている。\n",
    "    \"\"\"\n",
    "    # dims次元ベクトルに変換\n",
    "    dims = query_layer.shape[-1]\n",
    "    reshaped_queries = torch.reshape(query_layer, (-1,dims)).detach().numpy()\n",
    "    reshaped_keys = torch.reshape(key_layer, (-1,dims)).detach().numpy()\n",
    "\n",
    "    # コサイン距離\n",
    "    norm_queries = np.linalg.norm(reshaped_queries, axis=1)\n",
    "    norm_keys = np.linalg.norm(reshaped_keys, axis=1)\n",
    "    dot_product = calc_dot_product(query_layer, key_layer)\n",
    "    cosine_distance = 1 - dot_product / (norm_queries * norm_keys)\n",
    "    return cosine_distance\n",
    "\n",
    "def calc_weighted_corr(a, c, query_layer, key_layer):\n",
    "    \"\"\"重み付け相関。\n",
    "    これで正しいのかわからないけど、論文では単に「weighted correlation metric」と書いている。\n",
    "    このことを「正規化したベクトルを使った内積とコサイン距離の相関」と解釈して実装。\n",
    "    \"\"\"\n",
    "    normalized_query, normalized_key = normalize_vectors(query_layer, key_layer, a, c)\n",
    "    normalized_dot_product = calc_dot_product(normalized_query, normalized_key)\n",
    "    normalized_cosine_distance = calc_cosine_distance(normalized_query, normalized_key)\n",
    "\n",
    "    weighted_corr = np.corrcoef(normalized_dot_product, normalized_cosine_distance)\n",
    "    return weighted_corr[0, 1]\n",
    "\n",
    "\n",
    "# 定数a,cを適当な範囲の組み合わせで総当り。\n",
    "# aの範囲はもっと大きくしても動作し、より大きな相関が得られるが、描画結果は観察しづらくなる。\n",
    "# サンプル数の問題？\n",
    "optimal_a = optimal_c = None\n",
    "optimal_corr = 0\n",
    "\n",
    "for a in np.linspace(-0.5, 0.5, 10):\n",
    "#for a in [0]:\n",
    "    for c in np.linspace(-2,2, 10):\n",
    "        if c == 0.0:\n",
    "            continue\n",
    "        corr = calc_weighted_corr(a, c, query_layer, key_layer)\n",
    "        if np.abs(corr) > optimal_corr:\n",
    "            optimal_a = a\n",
    "            optimal_c = c\n",
    "            optimal_corr = np.abs(corr)\n",
    "            print(f'{optimal_a=}, {optimal_c=}, {optimal_corr=}')\n",
    "\n",
    "normalized_query, normalized_key = normalize_vectors(query_layer, key_layer, optimal_a, optimal_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正規化したベクトルにおける相関（コサイン距離 vs 内積）\n",
    "一つ手前のa,c検索時の出力をみると、初期値の時点でかなり相関の絶対値が大きく、その後ほとんど更新されていない。サンプル数が小さいこと、1レイヤーしか参照していないことが影響していると思うけれども、それ以上に今回の正規化アプローチが効果的であることを意味してるようにも見える。ただしここでいう効果的はあくまでも「コサイン距離と内積の相関を高く維持したまま調整しやすい」ぐらいの意味。ここでは実際に相関がどうだったのかを描画。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def plot_scatter_with_regression(scaled_cosine_distance, scaled_dot_product, optimal_a, optimal_c, optimal_corr):\n",
    "\n",
    "    # 回帰直線のフィッティング\n",
    "    def linear_fit(x, a, b):\n",
    "        return a * x + b\n",
    "    popt, _ = curve_fit(linear_fit, scaled_cosine_distance, scaled_dot_product)\n",
    "\n",
    "    # 散布図を作成\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=scaled_cosine_distance, y=scaled_dot_product,\n",
    "                             mode='markers', marker=dict(size=8), name='Data Points'))\n",
    "\n",
    "    # 回帰直線を追加\n",
    "    x_fit = np.linspace(min(scaled_cosine_distance), max(scaled_cosine_distance), 100)\n",
    "    y_fit = linear_fit(x_fit, *popt)\n",
    "    fig.add_trace(go.Scatter(x=x_fit, y=y_fit, mode='lines', line=dict(color='red', width=3), name='Regression Line'))\n",
    "\n",
    "    # レイアウト設定\n",
    "    fig.update_layout(title=f\"Scatter Plot with Regression Line (layer={layer_index}, a={optimal_a:.3f}, c={optimal_c:.3f}, abs(cor)={optimal_corr:.3f})\",\n",
    "                      xaxis_title=\"Scaled Cosine Distance\",\n",
    "                      yaxis_title=\"Scaled Dot Product\",\n",
    "                      showlegend=True)\n",
    "\n",
    "    # グラフを表示\n",
    "    #fig.show()\n",
    "    fig.write_image(f\"out/cor_layer{layer_index}.png\")\n",
    "\n",
    "normalized_dot_product = calc_dot_product(normalized_query, normalized_key)\n",
    "normalized_cosine_distance = calc_cosine_distance(normalized_query, normalized_key)\n",
    "plot_scatter_with_regression(normalized_cosine_distance, normalized_dot_product, optimal_a, optimal_c, optimal_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html><body><img src=\"./out/cor_layer0.png\"</body></html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(f'<html><body><img src=\"./out/cor_layer{layer_index}.png\"</body></html>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q,Kペアのアテンションを加味した位置関係（ヘッド毎に描画）\n",
    "やりたいことは「アテンションが高いペアをより近くに描画」すること。今回の正規化でどう変わるのかを比較するため、正規化前後でヘッド毎に別グラフを用意してみることに。\n",
    "\n",
    "- クエリは緑、キーはピンク。（論文通り）\n",
    "- クエリ＆キーのペアで最大アテンションとなる組み合わせを青色直線で結ぶ。直線の太さをアテンションに応じて調整。\n",
    "- このうち観察したいトークン（'brown' => 'cap', '##y', '##bara'）が最大ペアの場合には赤色とする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def plot2d_queries_and_keys_with_PCA(query_layer, key_layer, attention, comment):\n",
    "    \"\"\"クエリ＆キーのペアをPCAで描画。\n",
    "    図中の mean distance は、最大ペアのPCA空間における平均ユークリッド距離。\n",
    "    これが近くなることを期待しましたが、今回の結果ではそうならず。\n",
    "\n",
    "    Note:\n",
    "      (1) query_layer, key_layerには全シーケンス x 全隠れ層分の重みがそれぞれ入っているが、\n",
    "      ここでは決め打ちでシーケンス数1個目だけ参照、隠れ層は1番目だけ参照としている。\n",
    "\n",
    "    Args:\n",
    "      attention: アテンション行列（このファイル冒頭で取得）。\n",
    "      comment: ファイル名やキャプションに付けるための文字列。\n",
    "    \"\"\"\n",
    "    num_heads, num_tokens, embedding_dim = query_layer.shape[1:]\n",
    "    pca = PCA(n_components=2)\n",
    "\n",
    "    for head_index in range(num_heads):\n",
    "        # query_layerとkey_layerのベクトルを抽出して2次元に圧縮\n",
    "        query_vectors = query_layer[0, head_index].reshape(num_tokens, embedding_dim).detach().numpy() # 0 = sequence id\n",
    "        key_vectors = key_layer[0, head_index].reshape(num_tokens, embedding_dim).detach().numpy() # 0 = sequence id\n",
    "        all_vectors = np.concatenate([query_vectors, key_vectors])\n",
    "        pca.fit_transform(all_vectors)\n",
    "\n",
    "        # 描画の都合でクエリとキーに分ける\n",
    "        query_pca_result = pca.transform(query_vectors)\n",
    "        key_pca_result = pca.transform(key_vectors)\n",
    "\n",
    "        # 最もアテンションが大きいペアを探索\n",
    "        attention_head = attention[layer_index][0, head_index] # 0 = sequence id\n",
    "        max_attention_indices = torch.argmax(attention_head, dim=-1)\n",
    "        \n",
    "        # グラフを描画\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # query_layerの点を描画\n",
    "        query_trace = go.Scatter(x=query_pca_result[:, 0], y=query_pca_result[:, 1],\n",
    "                                mode='markers', name='query_layer',\n",
    "                                marker=dict(color='green', size=10), text=tokens, textposition='bottom center')\n",
    "\n",
    "        # key_layerの点を描画\n",
    "        key_trace = go.Scatter(x=key_pca_result[:, 0], y=key_pca_result[:, 1],\n",
    "                            mode='markers', name='key_layer',\n",
    "                            marker=dict(color='pink', size=10), text=tokens, textposition='top center')\n",
    "\n",
    "        # 各点に対応する文字列をアノテーションとして追加\n",
    "        for i in range(num_tokens):\n",
    "            query_text_annotation = dict(\n",
    "                x=query_pca_result[i, 0], y=query_pca_result[i, 1],\n",
    "                text=tokens[i], showarrow=False,\n",
    "                font=dict(size=12, color='green')\n",
    "            )\n",
    "            key_text_annotation = dict(\n",
    "                x=key_pca_result[i, 0], y=key_pca_result[i, 1],\n",
    "                text=tokens[i], showarrow=False,\n",
    "                font=dict(size=12, color='pink')\n",
    "            )\n",
    "            fig.add_annotation(**query_text_annotation)\n",
    "            fig.add_annotation(**key_text_annotation)\n",
    "\n",
    "        # 最もアテンションが大きいペアを直線で結ぶ\n",
    "        for i, max_idx in enumerate(max_attention_indices):\n",
    "            attettion_value = attention_head[i][max_idx]\n",
    "            query_point = query_pca_result[i]\n",
    "            key_point = key_pca_result[max_idx]\n",
    "            if i == 2 and (3 <= max_idx <=5):\n",
    "                color = \"red\"\n",
    "            else:\n",
    "                color = \"blue\"\n",
    "            fig.add_trace(go.Scatter(x=[query_point[0], key_point[0]], y=[query_point[1], key_point[1]],\n",
    "                                    mode='lines', line=dict(color=color, width=int(10*attettion_value)), showlegend=False))\n",
    "\n",
    "        # 2次元空間におけるユークリッド距離を計算\n",
    "        euclidean_distances = np.linalg.norm(query_pca_result - key_pca_result[max_attention_indices], axis=1)\n",
    "        mean_distance = np.mean(euclidean_distances)\n",
    "        \n",
    "        fig.update_layout(title=f\"{comment}: Head {head_index + 1} - Mean Distance: {mean_distance:.2f}\",\n",
    "                        xaxis_title=\"PCA Component 1\",\n",
    "                        yaxis_title=\"PCA Component 2\",\n",
    "                        showlegend=True)\n",
    "\n",
    "        #fig.show()\n",
    "        fig.write_image(f'./out/qk_{comment}_layer{layer_index}_head{head_index}.png')\n",
    "\n",
    "plot2d_queries_and_keys_with_PCA(query_layer, key_layer, attention, 'default')\n",
    "plot2d_queries_and_keys_with_PCA(normalized_query, normalized_key, attention, 'normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<html>\n",
       "<body>\n",
       "<table border=\"1\">\n",
       "<caption>Naive AttentionViz (bert-base-uncased, layer 0)</caption>\n",
       "<th>default</th><th>noramlized</th>\n",
       "<tr><td><img src=\"./out/qk_default_layer0_head0.png\"></td><td><img src=\"./out/qk_normalized_layer0_head0.png\"></td></tr>\n",
       "<tr><td><img src=\"./out/qk_default_layer0_head1.png\"></td><td><img src=\"./out/qk_normalized_layer0_head1.png\"></td></tr>\n",
       "<tr><td><img src=\"./out/qk_default_layer0_head2.png\"></td><td><img src=\"./out/qk_normalized_layer0_head2.png\"></td></tr>\n",
       "<tr><td><img src=\"./out/qk_default_layer0_head3.png\"></td><td><img src=\"./out/qk_normalized_layer0_head3.png\"></td></tr>\n",
       "<tr><td><img src=\"./out/qk_default_layer0_head4.png\"></td><td><img src=\"./out/qk_normalized_layer0_head4.png\"></td></tr>\n",
       "<tr><td><img src=\"./out/qk_default_layer0_head5.png\"></td><td><img src=\"./out/qk_normalized_layer0_head5.png\"></td></tr>\n",
       "<tr><td><img src=\"./out/qk_default_layer0_head6.png\"></td><td><img src=\"./out/qk_normalized_layer0_head6.png\"></td></tr>\n",
       "<tr><td><img src=\"./out/qk_default_layer0_head7.png\"></td><td><img src=\"./out/qk_normalized_layer0_head7.png\"></td></tr>\n",
       "<tr><td><img src=\"./out/qk_default_layer0_head8.png\"></td><td><img src=\"./out/qk_normalized_layer0_head8.png\"></td></tr>\n",
       "<tr><td><img src=\"./out/qk_default_layer0_head9.png\"></td><td><img src=\"./out/qk_normalized_layer0_head9.png\"></td></tr>\n",
       "<tr><td><img src=\"./out/qk_default_layer0_head10.png\"></td><td><img src=\"./out/qk_normalized_layer0_head10.png\"></td></tr>\n",
       "<tr><td><img src=\"./out/qk_default_layer0_head11.png\"></td><td><img src=\"./out/qk_normalized_layer0_head11.png\"></td></tr>\n",
       "</table></body></html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "html_text = f\"\"\"\n",
    "<html>\n",
    "<body>\n",
    "<table border=\"1\">\n",
    "<caption>Naive AttentionViz (bert-base-uncased, layer {layer_index})</caption>\n",
    "<th>default</th><th>noramlized</th>\n",
    "\"\"\"\n",
    "\n",
    "num_heads, num_tokens, embedding_dim = query_layer.shape[1:]\n",
    "for head_idx in range(num_heads):\n",
    "    html_text += f'<tr><td><img src=\"./out/qk_default_layer{layer_index}_head{head_idx}.png\"></td><td><img src=\"./out/qk_normalized_layer{layer_index}_head{head_idx}.png\"></td></tr>\\n'\n",
    "\n",
    "html_text += \"</table></body></html>\"\n",
    "\n",
    "with open(f\"./result_layer{layer_index}.html\", \"w\") as f:\n",
    "    f.write(html_text)\n",
    "\n",
    "HTML(html_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencalm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
